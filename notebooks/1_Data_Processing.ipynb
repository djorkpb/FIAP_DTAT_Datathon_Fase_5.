{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Etapa 1: Processamento e Consolidação dos Dados\n",
    "\n",
    "Neste notebook, vamos ler os três arquivos JSON brutos (`vagas`, `prospects`, `applicants`), limpá-los e padronizá-los na origem, uni-los numa única base de dados e aplicar os filtros de negócio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- ETAPA 1: PROCESSAMENTO E CONSOLIDAÇÃO DOS DADOS ---\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "import os\n",
    "\n",
    "print(\"--- ETAPA 1: PROCESSAMENTO E CONSOLIDAÇÃO DOS DADOS ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bloco 1: Limpeza do Arquivo de Vagas\n",
    "Este bloco executa a limpeza dos títulos e dos campos de idioma do arquivo `vagas.json` e cria o `vagas_cleaned.json`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processamento concluído. Vagas limpas salvas em '../data/processed/vagas_cleaned.json'.\n"
     ]
    }
   ],
   "source": [
    "def clean_title(title):\n",
    "    \"\"\"\n",
    "    Limpa o título de uma vaga removendo códigos numéricos e outros padrões\n",
    "    no início ou no final do texto.\n",
    "    \"\"\"\n",
    "    if not isinstance(title, str):\n",
    "        return \"\"\n",
    "    \n",
    "    # O padrão aceita espaços (\\s*) ao redor do hífen.\n",
    "    # Ex: \"2594750-Analista\" e \"4594852 - ABAP\" serão limpos.\n",
    "    cleaned_title = re.sub(r'^\\d+\\s*-\\s*', '', title)\n",
    "    \n",
    "    # Remove códigos numéricos ou alfanuméricos no final do título.\n",
    "    cleaned_title = re.sub(r'\\s+\\d{6,}[A-Z]*$', '', cleaned_title)\n",
    "    \n",
    "    return cleaned_title.strip()\n",
    "\n",
    "def process_vagas(input_path, output_path):\n",
    "    \"\"\"\n",
    "    Lê um arquivo JSON de vagas, limpa os títulos e salva em um novo arquivo.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(input_path, 'r', encoding='utf-8') as f:\n",
    "            vagas_data = json.load(f)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Erro: O arquivo de entrada '{input_path}' não foi encontrado.\")\n",
    "        return\n",
    "    except json.JSONDecodeError:\n",
    "        print(f\"Erro: O arquivo '{input_path}' não é um JSON válido.\")\n",
    "        return\n",
    "\n",
    "    cleaned_vagas = {}\n",
    "    for vaga_id, data in vagas_data.items():\n",
    "        cleaned_data = data.copy()\n",
    "        \n",
    "        # Limpeza de Títulos\n",
    "        if 'informacoes_basicas' in cleaned_data and isinstance(cleaned_data.get('informacoes_basicas'), dict) and 'titulo_vaga' in cleaned_data['informacoes_basicas']:\n",
    "            original_title = cleaned_data['informacoes_basicas']['titulo_vaga']\n",
    "            # Aplica a função de limpeza corrigida\n",
    "            cleaned_data['informacoes_basicas']['titulo_vaga'] = clean_title(original_title)\n",
    "        \n",
    "        # Limpeza e Padronização dos Níveis de Idioma\n",
    "        if 'perfil_vaga' in cleaned_data and isinstance(cleaned_data.get('perfil_vaga'), dict):\n",
    "            perfil = cleaned_data['perfil_vaga']\n",
    "            \n",
    "            # Trata nível de inglês\n",
    "            nivel_ingles = perfil.get('nivel_ingles')\n",
    "            if nivel_ingles is None or not str(nivel_ingles).strip():\n",
    "                perfil['nivel_ingles'] = 'Nenhum'\n",
    "            else:\n",
    "                perfil['nivel_ingles'] = str(nivel_ingles).strip()\n",
    "            \n",
    "            # Trata nível de espanhol\n",
    "            nivel_espanhol = perfil.get('nivel_espanhol')\n",
    "            if nivel_espanhol is None or not str(nivel_espanhol).strip():\n",
    "                perfil['nivel_espanhol'] = 'Nenhum'\n",
    "            else:\n",
    "                perfil['nivel_espanhol'] = str(nivel_espanhol).strip()\n",
    "                \n",
    "        cleaned_vagas[vaga_id] = cleaned_data\n",
    "\n",
    "    try:\n",
    "        output_dir = os.path.dirname(output_path)\n",
    "        if output_dir:\n",
    "            os.makedirs(output_dir, exist_ok=True)\n",
    "            \n",
    "        with open(output_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(cleaned_vagas, f, ensure_ascii=False, indent=4)\n",
    "        print(f\"Processamento concluído. Vagas limpas salvas em '{output_path}'.\")\n",
    "    except IOError as e:\n",
    "        print(f\"Erro ao escrever no arquivo de saída '{output_path}': {e}\")\n",
    "\n",
    "VAGAS_RAW_PATH = '../data/raw/vagas.json'\n",
    "VAGAS_PROCESSED_PATH = '../data/processed/vagas_cleaned.json'\n",
    "process_vagas(VAGAS_RAW_PATH, VAGAS_PROCESSED_PATH)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bloco 2: Carregar `vagas_cleaned.json` para DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Bloco 2: Estruturando vagas com método padronizado ---\n",
      "DataFrame de vagas criado com 14081 linhas e 45 colunas.\n",
      "\n",
      "Amostra dos nomes das colunas:\n",
      "['id_vaga', 'informacoes_basicas.data_requicisao', 'informacoes_basicas.limite_esperado_para_contratacao', 'informacoes_basicas.titulo_vaga', 'informacoes_basicas.vaga_sap', 'informacoes_basicas.cliente', 'informacoes_basicas.solicitante_cliente', 'informacoes_basicas.empresa_divisao', 'informacoes_basicas.requisitante', 'informacoes_basicas.analista_responsavel', 'informacoes_basicas.tipo_contratacao', 'informacoes_basicas.prazo_contratacao', 'informacoes_basicas.objetivo_vaga', 'informacoes_basicas.prioridade_vaga', 'informacoes_basicas.origem_vaga']\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Bloco 2: Estruturando vagas com método padronizado ---\")\n",
    "\n",
    "# Carrega o arquivo JSON de vagas já limpo\n",
    "with open('../data/processed/vagas_cleaned.json', 'r', encoding='utf-8') as f:\n",
    "    vagas_data = json.load(f)\n",
    "\n",
    "# Converte o dicionário para um DataFrame com o ID como índice e colunas aninhadas\n",
    "vagas_raw_df = pd.DataFrame.from_dict(vagas_data, orient='index')\n",
    "vagas_raw_df.index.name = 'id_vaga'\n",
    "\n",
    "# Define as colunas aninhadas que serão achatadas (mesma abordagem dos applicants)\n",
    "nested_columns = ['informacoes_basicas', 'perfil_vaga', 'beneficios']\n",
    "df_parts = [vagas_raw_df.drop(columns=nested_columns, errors='ignore')]\n",
    "\n",
    "# Itera sobre cada coluna aninhada, normaliza e adiciona o prefixo\n",
    "for col in nested_columns:\n",
    "    if col in vagas_raw_df.columns:\n",
    "        # Garante que a coluna não contenha apenas valores nulos antes de normalizar\n",
    "        series_no_na = vagas_raw_df[col].dropna()\n",
    "        if not series_no_na.empty and isinstance(series_no_na.iloc[0], dict):\n",
    "            # Normaliza a seção e adiciona o nome da seção como prefixo\n",
    "            normalized_part = pd.json_normalize(vagas_raw_df[col]).add_prefix(f\"{col}.\")\n",
    "            normalized_part.index = vagas_raw_df.index\n",
    "            df_parts.append(normalized_part)\n",
    "\n",
    "# Concatena as partes (colunas originais + colunas achatadas)\n",
    "vagas_df = pd.concat(df_parts, axis=1)\n",
    "vagas_df.reset_index(inplace=True)\n",
    "\n",
    "print(f\"DataFrame de vagas criado com {vagas_df.shape[0]} linhas e {vagas_df.shape[1]} colunas.\")\n",
    "\n",
    "# Exibe as primeiras colunas para verificação do novo padrão\n",
    "print(\"\\nAmostra dos nomes das colunas:\")\n",
    "print(vagas_df.columns.tolist()[:15])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bloco 3: Processar `prospects.json`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processando prospects.json...\n",
      "prospects.json processado. 53759 candidaturas encontradas.\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nProcessando prospects.json...\")\n",
    "with open('../data/raw/prospects.json', 'r', encoding='utf-8') as f:\n",
    "    prospects_data = json.load(f)\n",
    "\n",
    "prospects_list = []\n",
    "for vaga_id, data in prospects_data.items():\n",
    "    for prospect in data.get('prospects', []):\n",
    "        if prospect: \n",
    "            prospect_info = {\n",
    "                'id_vaga': vaga_id,\n",
    "                **prospect\n",
    "            }\n",
    "            prospects_list.append(prospect_info)\n",
    "\n",
    "prospects_df = pd.DataFrame(prospects_list).add_prefix('prospect.')\n",
    "prospects_df.rename(columns={'prospect.id_vaga': 'id_vaga'}, inplace=True)\n",
    "print(f\"prospects.json processado. {len(prospects_df)} candidaturas encontradas.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bloco 4: Processar `applicants.json` e Aplicar Filtros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processando applicants.json...\n",
      "Níveis de idioma dos applicants foram limpos e padronizados (antes do flatten).\n",
      "Dados dos candidatos foram achatados (flattened).\n",
      "Aplicando filtro de qualificações no CV...\n",
      "34447 candidatos removidos.\n",
      "8035 candidatos válidos restantes.\n",
      "\n",
      "Guardando applicants_cleaned.json ---\n",
      "arquivo '../data/processed/applicants_cleaned.json' guardado com sucesso.\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nProcessando applicants.json...\")\n",
    "applicants_raw_df = pd.read_json('../data/raw/applicants.json', orient='index')\n",
    "applicants_raw_df.index.name = 'id_candidato'\n",
    "\n",
    "# Limpeza e padronização dos níveis de idioma (antes do flatten)\n",
    "def clean_applicant_languages(row):\n",
    "    if isinstance(row['formacao_e_idiomas'], dict):\n",
    "        formacao = row['formacao_e_idiomas'].copy()\n",
    "        for lang in ['nivel_ingles', 'nivel_espanhol']:\n",
    "            nivel = formacao.get(lang)\n",
    "            if nivel is None or not str(nivel).strip():\n",
    "                formacao[lang] = 'Nenhum'\n",
    "            else:\n",
    "                formacao[lang] = str(nivel).strip()\n",
    "        return formacao\n",
    "    return row['formacao_e_idiomas']\n",
    "\n",
    "if 'formacao_e_idiomas' in applicants_raw_df.columns:\n",
    "    applicants_raw_df['formacao_e_idiomas'] = applicants_raw_df.apply(clean_applicant_languages, axis=1)\n",
    "    print(\"Níveis de idioma dos applicants foram limpos e padronizados (antes do flatten).\")\n",
    "else:\n",
    "    print(\"Aviso: Coluna 'formacao_e_idiomas' não encontrada. A limpeza não foi aplicada.\")\n",
    "\n",
    "# Achatamento (flattening) dos dados do candidato\n",
    "nested_columns = [\n",
    "    'infos_basicas', 'informacoes_pessoais', 'informacoes_profissionais', \n",
    "    'formacao_e_idiomas', 'cargo_atual'\n",
    "]\n",
    "df_parts = [applicants_raw_df.drop(columns=nested_columns, errors='ignore')]\n",
    "\n",
    "for col in nested_columns:\n",
    "    if col in applicants_raw_df.columns:\n",
    "        series_no_na = applicants_raw_df[col].dropna()\n",
    "        if not series_no_na.empty and isinstance(series_no_na.iloc[0], dict):\n",
    "            normalized_part = pd.json_normalize(applicants_raw_df[col]).add_prefix(f\"{col}.\")\n",
    "            normalized_part.index = applicants_raw_df.index\n",
    "            df_parts.append(normalized_part)\n",
    "\n",
    "applicants_df = pd.concat(df_parts, axis=1)\n",
    "applicants_df.reset_index(inplace=True)\n",
    "print(\"Dados dos candidatos foram achatados (flattened).\")\n",
    "\n",
    "# REGRA DE NEGÓCIO - Filtro de negócio: manter apenas candidatos que mencionam 'qualificações' no CV\n",
    "print(\"Aplicando filtro de qualificações no CV...\")\n",
    "initial_candidates = len(applicants_df)\n",
    "if 'cv_pt' in applicants_df.columns:\n",
    "    mask = applicants_df['cv_pt'].notna() & applicants_df['cv_pt'].str.contains(\n",
    "        'qualificaç(?:ão|ões)',\n",
    "        case=False, \n",
    "        na=False, \n",
    "        regex=True\n",
    "    )\n",
    "    applicants_filtered_df = applicants_df[mask].copy()\n",
    "    print(f\"{initial_candidates - len(applicants_filtered_df)} candidatos removidos.\")\n",
    "    print(f\"{len(applicants_filtered_df)} candidatos válidos restantes.\")\n",
    "else:\n",
    "    print(\"Aviso: Coluna 'cv_pt' não encontrada. O filtro de qualificações não foi aplicado.\")\n",
    "    applicants_filtered_df = applicants_df\n",
    "\n",
    "print(\"\\nGuardando applicants_cleaned.json ---\")\n",
    "APPLICANTS_PROCESSED_PATH = '../data/processed/applicants_cleaned.json'\n",
    "\n",
    "# Para guardar no mesmo formato original (JSON com ID como chave), definimos o ID como índice\n",
    "applicants_to_save_df = applicants_df.set_index('id_candidato')\n",
    "applicants_to_save_df.to_json(APPLICANTS_PROCESSED_PATH, orient='index', indent=4, force_ascii=False)\n",
    "\n",
    "print(f\"arquivo '{APPLICANTS_PROCESSED_PATH}' guardado com sucesso.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bloco 5: Unificar as Bases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Unificando os dados...\n",
      "Unificação concluída.\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nUnificando os dados...\")\n",
    "# Garante que as chaves de união sejam do mesmo tipo (string)\n",
    "prospects_df['id_vaga'] = prospects_df['id_vaga'].astype(str)\n",
    "vagas_df['id_vaga'] = vagas_df['id_vaga'].astype(str)\n",
    "prospects_df['prospect.codigo'] = prospects_df['prospect.codigo'].astype(str)\n",
    "applicants_filtered_df['id_candidato'] = applicants_filtered_df['id_candidato'].astype(str)\n",
    "\n",
    "dados_consolidados = pd.merge(prospects_df, vagas_df, on='id_vaga', how='left')\n",
    "dados_consolidados = pd.merge(dados_consolidados, applicants_filtered_df, left_on='prospect.codigo', right_on='id_candidato', how='inner')\n",
    "print(\"Unificação concluída.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bloco 6: Criar a Variável-Alvo (`match`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Criando a variável-alvo 'match'...\n",
      "Variável-alvo criada.\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nCriando a variável-alvo 'match'...\")\n",
    "dados_consolidados['match'] = (dados_consolidados['prospect.situacao_candidado'] == 'Contratado pela Decision').astype(int)\n",
    "print(\"Variável-alvo criada.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bloco Final: Análise e Salvamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- BASE DE DADOS CONSOLIDADA ---\n",
      "Dimensões da base final: 13846 linhas, 111 colunas\n",
      "\n",
      "Distribuição da variável-alvo:\n",
      "match\n",
      "0    13073\n",
      "1      773\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Arquivo '../data/processed/dados_consolidados.json' salvo com sucesso!\n",
      "\n",
      "--- ETAPA 1 CONCLUÍDA ---\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- BASE DE DADOS CONSOLIDADA ---\")\n",
    "print(f\"Dimensões da base final: {dados_consolidados.shape[0]} linhas, {dados_consolidados.shape[1]} colunas\")\n",
    "\n",
    "print(\"\\nDistribuição da variável-alvo:\")\n",
    "print(dados_consolidados['match'].value_counts())\n",
    "\n",
    "dados_consolidados.to_json('../data/processed/dados_consolidados.json', orient='records', lines=True, force_ascii=False)\n",
    "print(\"\\nArquivo '../data/processed/dados_consolidados.json' salvo com sucesso!\")\n",
    "print(\"\\n--- ETAPA 1 CONCLUÍDA ---\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
