{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Etapa 1: Processamento e Consolidação dos Dados\n",
    "\n",
    "Neste notebook, vamos ler os três arquivos JSON brutos (`vagas`, `prospects`, `applicants`), limpá-los e padronizá-los na origem, uni-los numa única base de dados e aplicar os filtros de negócio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- ETAPA 1: PROCESSAMENTO E CONSOLIDAÇÃO DOS DADOS ---\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "import os\n",
    "\n",
    "print(\"--- ETAPA 1: PROCESSAMENTO E CONSOLIDAÇÃO DOS DADOS ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bloco 1: Limpeza do Arquivo de Vagas\n",
    "Este bloco executa a limpeza dos títulos e dos campos de idioma do arquivo `vagas.json` e cria o `vagas_cleaned.json`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lendo o arquivo de vagas de: ../data/raw/vagas.json\n",
      "Total de 14081 vagas encontradas. Iniciando limpeza...\n",
      "Limpeza concluída. Arquivo salvo em: ../data/processed/vagas_cleaned.json\n"
     ]
    }
   ],
   "source": [
    "def clean_vagas_json(input_path='../data/raw/vagas.json', output_path='../data/processed/vagas_cleaned.json'):\n",
    "    print(f\"Lendo o arquivo de vagas de: {input_path}\")\n",
    "    with open(input_path, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    def clean_title(title):\n",
    "        # Passo 0: Tratar valores nulos (null no JSON) ou não-strings.\n",
    "        if title is None or not isinstance(title, str) or not title.strip():\n",
    "            return \"Título Indisponível\"\n",
    "        \n",
    "        try:\n",
    "            text = str(title).strip()\n",
    "\n",
    "            # Passo 1: Remover prefixos de código de forma segura.\n",
    "            # Encontra a primeira letra e remove tudo o que vier antes se contiver um número.\n",
    "            match = re.search(r'[a-zA-Z]', text)\n",
    "            if match:\n",
    "                first_letter_index = match.start()\n",
    "                if first_letter_index > 0:\n",
    "                    prefix = text[:first_letter_index]\n",
    "                    if re.search(r'\\d', prefix):\n",
    "                        text = text[first_letter_index:]\n",
    "\n",
    "            # Passo 2: Remover sufixos de código de forma segura.\n",
    "            # Remove padrões que se parecem claramente com códigos de requisição no final.\n",
    "            text = re.sub(r'\\s*[-–—]\\s*([A-Za-z0-9-]*\\d+[A-Za-z0-9-]*)$', '', text).strip()\n",
    "            # Remove também sufixos que são apenas números longos.\n",
    "            text = re.sub(r'\\s+\\d{4,}$', '', text).strip()\n",
    "\n",
    "            # Passo 3: Limpeza final de caracteres residuais.\n",
    "            # Remove quaisquer hífens, asteriscos ou espaços que sobraram nas pontas.\n",
    "            text = text.strip(' *–-_—.,:')\n",
    "\n",
    "            # Passo 4: Garantia final contra resultado nulo ou vazio.\n",
    "            # Se a limpeza removeu tudo, retorna um placeholder.\n",
    "            if not text:\n",
    "                return \"Título Indisponível\"\n",
    "\n",
    "            return text\n",
    "\n",
    "        except Exception as e:\n",
    "            # Se qualquer passo falhar, captura a exceção e retorna um placeholder.\n",
    "            return \"Título Indisponível\"\n",
    "\n",
    "    total_vagas = len(data)\n",
    "    print(f\"Total de {total_vagas} vagas encontradas. Iniciando limpeza...\")\n",
    "    \n",
    "    for vaga_id, vaga_details in data.items():\n",
    "        # Limpeza do Título da Vaga\n",
    "        if 'informacoes_basicas' in vaga_details and 'titulo_vaga' in vaga_details['informacoes_basicas']:\n",
    "            original_title = vaga_details['informacoes_basicas']['titulo_vaga']\n",
    "            cleaned_title = clean_title(original_title)\n",
    "            vaga_details['informacoes_basicas']['titulo_vaga'] = cleaned_title\n",
    "\n",
    "        # Limpeza e Padronização dos Níveis de Idioma\n",
    "        if 'perfil_vaga' in vaga_details:\n",
    "            # Inglês\n",
    "            nivel_ingles = vaga_details['perfil_vaga'].get('nivel_ingles')\n",
    "            if nivel_ingles is None or str(nivel_ingles).strip() == '':\n",
    "                vaga_details['perfil_vaga']['nivel_ingles'] = 'Nenhum'\n",
    "            else:\n",
    "                vaga_details['perfil_vaga']['nivel_ingles'] = str(nivel_ingles).strip()\n",
    "            \n",
    "            # Espanhol\n",
    "            nivel_espanhol = vaga_details['perfil_vaga'].get('nivel_espanhol')\n",
    "            if nivel_espanhol is None or str(nivel_espanhol).strip() == '':\n",
    "                vaga_details['perfil_vaga']['nivel_espanhol'] = 'Nenhum'\n",
    "            else:\n",
    "                vaga_details['perfil_vaga']['nivel_espanhol'] = str(nivel_espanhol).strip()\n",
    "\n",
    "    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "    with open(output_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(data, f, ensure_ascii=False, indent=4)\n",
    "    \n",
    "    print(f\"Limpeza concluída. Arquivo salvo em: {output_path}\")\n",
    "\n",
    "# Executa a função\n",
    "clean_vagas_json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bloco 2: Carregar `vagas_cleaned.json` para DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Bloco 2: Estruturando vagas com método padronizado ---\n",
      "DataFrame de vagas criado com 14081 linhas e 45 colunas.\n",
      "\n",
      "Amostra dos nomes das colunas:\n",
      "['id_vaga', 'informacoes_basicas.data_requicisao', 'informacoes_basicas.limite_esperado_para_contratacao', 'informacoes_basicas.titulo_vaga', 'informacoes_basicas.vaga_sap', 'informacoes_basicas.cliente', 'informacoes_basicas.solicitante_cliente', 'informacoes_basicas.empresa_divisao', 'informacoes_basicas.requisitante', 'informacoes_basicas.analista_responsavel', 'informacoes_basicas.tipo_contratacao', 'informacoes_basicas.prazo_contratacao', 'informacoes_basicas.objetivo_vaga', 'informacoes_basicas.prioridade_vaga', 'informacoes_basicas.origem_vaga']\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Bloco 2: Estruturando vagas com método padronizado ---\")\n",
    "\n",
    "# Carrega o arquivo JSON de vagas já limpo\n",
    "with open('../data/processed/vagas_cleaned.json', 'r', encoding='utf-8') as f:\n",
    "    vagas_data = json.load(f)\n",
    "\n",
    "# Converte o dicionário para um DataFrame com o ID como índice e colunas aninhadas\n",
    "vagas_raw_df = pd.DataFrame.from_dict(vagas_data, orient='index')\n",
    "vagas_raw_df.index.name = 'id_vaga'\n",
    "\n",
    "# Define as colunas aninhadas que serão achatadas (mesma abordagem dos applicants)\n",
    "nested_columns = ['informacoes_basicas', 'perfil_vaga', 'beneficios']\n",
    "df_parts = [vagas_raw_df.drop(columns=nested_columns, errors='ignore')]\n",
    "\n",
    "# Itera sobre cada coluna aninhada, normaliza e adiciona o prefixo\n",
    "for col in nested_columns:\n",
    "    if col in vagas_raw_df.columns:\n",
    "        # Garante que a coluna não contenha apenas valores nulos antes de normalizar\n",
    "        series_no_na = vagas_raw_df[col].dropna()\n",
    "        if not series_no_na.empty and isinstance(series_no_na.iloc[0], dict):\n",
    "            # Normaliza a seção e adiciona o nome da seção como prefixo\n",
    "            normalized_part = pd.json_normalize(vagas_raw_df[col]).add_prefix(f\"{col}.\")\n",
    "            normalized_part.index = vagas_raw_df.index\n",
    "            df_parts.append(normalized_part)\n",
    "\n",
    "# Concatena as partes (colunas originais + colunas achatadas)\n",
    "vagas_df = pd.concat(df_parts, axis=1)\n",
    "vagas_df.reset_index(inplace=True)\n",
    "\n",
    "print(f\"DataFrame de vagas criado com {vagas_df.shape[0]} linhas e {vagas_df.shape[1]} colunas.\")\n",
    "\n",
    "# Exibe as primeiras colunas para verificação do novo padrão\n",
    "print(\"\\nAmostra dos nomes das colunas:\")\n",
    "print(vagas_df.columns.tolist()[:15])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bloco 3: Processar `prospects.json`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processando prospects.json...\n",
      "prospects.json processado. 53759 candidaturas encontradas.\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nProcessando prospects.json...\")\n",
    "with open('../data/raw/prospects.json', 'r', encoding='utf-8') as f:\n",
    "    prospects_data = json.load(f)\n",
    "\n",
    "prospects_list = []\n",
    "for vaga_id, data in prospects_data.items():\n",
    "    for prospect in data.get('prospects', []):\n",
    "        if prospect: \n",
    "            prospect_info = {\n",
    "                'id_vaga': vaga_id,\n",
    "                **prospect\n",
    "            }\n",
    "            prospects_list.append(prospect_info)\n",
    "\n",
    "prospects_df = pd.DataFrame(prospects_list).add_prefix('prospect.')\n",
    "prospects_df.rename(columns={'prospect.id_vaga': 'id_vaga'}, inplace=True)\n",
    "print(f\"prospects.json processado. {len(prospects_df)} candidaturas encontradas.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bloco 4: Processar `applicants.json`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processando applicants.json...\n",
      "Níveis de idioma dos applicants foram limpos e padronizados (antes do flatten).\n",
      "Dados dos candidatos foram achatados (flattened).\n",
      "Total de candidatos que permanecem na base: 42482\n",
      "\n",
      "Guardando applicants_cleaned.json ---\n",
      "arquivo '../data/processed/applicants_cleaned.json' guardado com sucesso.\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nProcessando applicants.json...\")\n",
    "applicants_raw_df = pd.read_json('../data/raw/applicants.json', orient='index')\n",
    "applicants_raw_df.index.name = 'id_candidato'\n",
    "\n",
    "# Limpeza e padronização dos níveis de idioma (antes do flatten)\n",
    "def clean_applicant_languages(row):\n",
    "    if isinstance(row['formacao_e_idiomas'], dict):\n",
    "        formacao = row['formacao_e_idiomas'].copy()\n",
    "        for lang in ['nivel_ingles', 'nivel_espanhol']:\n",
    "            nivel = formacao.get(lang)\n",
    "            if nivel is None or not str(nivel).strip():\n",
    "                formacao[lang] = 'Nenhum'\n",
    "            else:\n",
    "                formacao[lang] = str(nivel).strip()\n",
    "        return formacao\n",
    "    return row['formacao_e_idiomas']\n",
    "\n",
    "if 'formacao_e_idiomas' in applicants_raw_df.columns:\n",
    "    applicants_raw_df['formacao_e_idiomas'] = applicants_raw_df.apply(clean_applicant_languages, axis=1)\n",
    "    print(\"Níveis de idioma dos applicants foram limpos e padronizados (antes do flatten).\")\n",
    "else:\n",
    "    print(\"Aviso: Coluna 'formacao_e_idiomas' não encontrada. A limpeza não foi aplicada.\")\n",
    "\n",
    "# Achatamento (flattening) dos dados do candidato\n",
    "nested_columns = [\n",
    "    'infos_basicas', 'informacoes_pessoais', 'informacoes_profissionais', \n",
    "    'formacao_e_idiomas', 'cargo_atual'\n",
    "]\n",
    "df_parts = [applicants_raw_df.drop(columns=nested_columns, errors='ignore')]\n",
    "\n",
    "for col in nested_columns:\n",
    "    if col in applicants_raw_df.columns:\n",
    "        series_no_na = applicants_raw_df[col].dropna()\n",
    "        if not series_no_na.empty and isinstance(series_no_na.iloc[0], dict):\n",
    "            normalized_part = pd.json_normalize(applicants_raw_df[col]).add_prefix(f\"{col}.\")\n",
    "            normalized_part.index = applicants_raw_df.index\n",
    "            df_parts.append(normalized_part)\n",
    "\n",
    "applicants_df = pd.concat(df_parts, axis=1)\n",
    "applicants_df.reset_index(inplace=True)\n",
    "print(\"Dados dos candidatos foram achatados (flattened).\")\n",
    "\n",
    "print(f\"Total de candidatos que permanecem na base: {len(applicants_df)}\")\n",
    "\n",
    "print(\"\\nGuardando applicants_cleaned.json ---\")\n",
    "APPLICANTS_PROCESSED_PATH = '../data/processed/applicants_cleaned.json'\n",
    "\n",
    "# Para guardar no mesmo formato original (JSON com ID como chave), definimos o ID como índice\n",
    "applicants_to_save_df = applicants_df.set_index('id_candidato')\n",
    "applicants_to_save_df.to_json(APPLICANTS_PROCESSED_PATH, orient='index', indent=4, force_ascii=False)\n",
    "\n",
    "print(f\"arquivo '{APPLICANTS_PROCESSED_PATH}' guardado com sucesso.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bloco 5: Unificar as Bases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Unificando os dados...\n",
      "Unificação concluída.\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nUnificando os dados...\")\n",
    "# Garante que as chaves de união sejam do mesmo tipo (string)\n",
    "prospects_df['id_vaga'] = prospects_df['id_vaga'].astype(str)\n",
    "vagas_df['id_vaga'] = vagas_df['id_vaga'].astype(str)\n",
    "prospects_df['prospect.codigo'] = prospects_df['prospect.codigo'].astype(str)\n",
    "applicants_df['id_candidato'] = applicants_df['id_candidato'].astype(str)\n",
    "\n",
    "dados_consolidados = pd.merge(prospects_df, vagas_df, on='id_vaga', how='left')\n",
    "dados_consolidados = pd.merge(dados_consolidados, applicants_df, left_on='prospect.codigo', right_on='id_candidato', how='inner')\n",
    "print(\"Unificação concluída.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bloco 6: Criar a Variável-Alvo (`match`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Criando a variável-alvo 'match'...\n",
      "Variável-alvo criada.\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nCriando a variável-alvo 'match'...\")\n",
    "dados_consolidados['match'] = (dados_consolidados['prospect.situacao_candidado'] == 'Contratado pela Decision').astype(int)\n",
    "print(\"Variável-alvo criada.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bloco Final: Análise e Salvamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- BASE DE DADOS CONSOLIDADA ---\n",
      "Dimensões da base final: 45095 linhas, 111 colunas\n",
      "\n",
      "Distribuição da variável-alvo:\n",
      "match\n",
      "0    42840\n",
      "1     2255\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Arquivo '../data/processed/dados_consolidados.json' salvo com sucesso!\n",
      "\n",
      "--- ETAPA 1 CONCLUÍDA ---\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- BASE DE DADOS CONSOLIDADA ---\")\n",
    "print(f\"Dimensões da base final: {dados_consolidados.shape[0]} linhas, {dados_consolidados.shape[1]} colunas\")\n",
    "\n",
    "print(\"\\nDistribuição da variável-alvo:\")\n",
    "print(dados_consolidados['match'].value_counts())\n",
    "\n",
    "dados_consolidados.to_json('../data/processed/dados_consolidados.json', orient='records', lines=True, force_ascii=False)\n",
    "print(\"\\nArquivo '../data/processed/dados_consolidados.json' salvo com sucesso!\")\n",
    "print(\"\\n--- ETAPA 1 CONCLUÍDA ---\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
